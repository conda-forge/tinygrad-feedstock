context:
  version: "0.11.1dev"
  build_number: 0
  # see github.com/conda-forge/conda-forge.github.io/issues/1059 for naming discussion
  proc_type: ${{ "cuda" ~ cuda_compiler_version | version_to_buildstring if cuda_compiler_version != "None" else "cpu" }}

recipe:
  name: tinygrad
  version: ${{ version }}

source:
  - git: https://github.com/tinygrad/tinygrad.git
    rev: "54af29dbdb6c266bae2590e68d4a0b2c5e61279c"
    # url: https://github.com/tinygrad/tinygrad/archive/refs/tags/v${{ version }}.tar.gz
    # sha256: e5e7b0f2319a8cc8e5de14dbc6ee91a403efd2414681b874b526a8b39d7fa86f
    patches:
      - patches/0001-point-to-our-own-llvmdev.patch
      # see more clearly what's happening in mnist test
      - patches/0002-update-accuracy-for-every-step-in-beautiful_mnist.py.patch
      # ensure we find vendored applegpu
      - patches/0003-make-apple-disassembler-runnable-when-installed.patch
      - if: osx and x86_64
        then:
          # disable metal on osx
          - patches/0004-default-to-CPU-as-device-on-osx-64-remove-metal.patch
      - if: osx and arm64
        then:
          # cannot disassemble metal when using vanilla clang; see
          # https://github.com/tinygrad/tinygrad/issues/14005
          - patches/0005-skip-metal-disassembly-for-vanilla-clang.patch
      # help nvcc find target-specific headers
      - patches/0006-add-target_dir-based-include-paths-for-CUDA-compiler.patch
  - if: osx and arm64
    then:
      - target_directory: tinygrad/extra/disassemblers/applegpu
        git: https://github.com/dougallj/applegpu
        rev: "797862eea0cee1f1eba74be3d0d02be4b3d2bd0d"

build:
  number: ${{ build_number }}
  string: ${{ proc_type }}_py${{ python | version_to_buildstring }}_h${{ hash }}_${{ build_number }}
  skip:
    # CUDA builds on GPU agents, non-CUDA builds on CPU agents; skip could be avoided
    # using https://github.com/conda-forge/conda-forge-pinning-feedstock/pull/6910
    - cuda_compiler_version == "None" and "gpu" in github_actions_labels
    - cuda_compiler_version != "None" and "cpu" in github_actions_labels

outputs:
  - package:
      name: tinygrad
    build:
      script:
        - python -m pip install . -vv
        - if: osx and arm64
          then:
            - mkdir -p $SP_DIR/tinygrad/extra/disassemblers/applegpu
            - cp -R ./tinygrad/extra/disassemblers/applegpu/* $SP_DIR/tinygrad/extra/disassemblers/applegpu
            - cd $SP_DIR/tinygrad/extra/disassemblers/applegpu/compiler_explorer_tools
            - make
    requirements:
      build:
        - if: build_platform != host_platform
          then:
            - python
            - cross-python_${{ host_platform }}
        # this is just here so smithy creates separate jobs for the CUDA variants
        - if: cuda_compiler_version != "None"
          then:
            - ${{ stdlib("c") }}
            - ${{ compiler("cxx") }}
            - ${{ compiler("cuda") }}
        - if: osx and arm64
          then:
            - make
            # for bare clang/clang++ symlinks, see
            # https://github.com/dougallj/applegpu/blob/main/compiler_explorer_tools/Makefile
            - clang
            - clangxx
      host:
        - python
        - pip
        - setuptools
      run:
        - python
        - capstone
        - llvmdev
        - clangxx_${{ host_platform }}
        # tinygrad wants headers from `include/clang-c`; also ensure `bin/clang` symlink is present
        - clangdev
        # avoid solver choosing ancient clang versions
        - clang >18
        - if: cuda_compiler_version != "None"
          then:
            - __cuda
            - cuda-nvcc_${{ host_platform }}
            - cuda-nvrtc-dev
            - if: linux
              then:
                - triton
      run_constraints:
        - if: linux
          then:
            # avoid wrong compiler activation that interferes with clang_<target>
            - gcc_linux-64 <0.0a0
            - gxx_linux-64 <0.0a0
            - gcc_linux-aarch64 <0.0a0
            - gxx_linux-aarch64 <0.0a0
        - if: win
          then:
            # avoid pulling in old MSVC activation
            - vs2019_win-64 <0.0a0
    tests:
      - python:
          imports:
            - tinygrad
            - tinygrad.runtime.autogen.llvm
          pip_check: true
      - script:
          - if: linux
            then:
              # cannot do a bare import test, because libc.syscall is a member, not a package
              - python -c "from tinygrad.runtime.autogen.libc import syscall"
            else:
              - echo nothing
      # integration-test in addition to test suite below
      - files:
          source:
            - examples/beautiful_mnist.py
        script:
          # reduce number of steps; we just need to show that it's functional
          # however, accuracy should at least trend in the right direction
          - if: unix
            then:
              - export STEPS=25
              - export TARGET_EVAL_ACC_PCT=90
            else:
              - set "STEPS=25"
              - set "TARGET_EVAL_ACC_PCT=90"
          - if: aarch64
            then:
              # reduce number of steps in emulation
              - export STEPS=3
              - export TARGET_EVAL_ACC_PCT=30
          - python ./examples/beautiful_mnist.py
          # run again with high DEBUG value; exercises different paths, e.g. needs capstone
          - if: unix
            then:
              - export STEPS=1
              - export DEBUG=10
              - unset TARGET_EVAL_ACC_PCT
            else:
              - set "STEPS=1"
              - set "DEBUG=10"
              - set "TARGET_EVAL_ACC_PCT="
          - if: not aarch64
            then:
              # suppress enormously noisy output; re-enable if things actually fail
              - python ./examples/beautiful_mnist.py > /dev/null

  - package:
      name: tinygrad-tests
    build:
      # splitting off what conda packages under etc/conda/test-files when using test.files.source
      script:
        - if: unix
          # silly: https://github.com/prefix-dev/rattler-build/pull/1367
          then: true
          else:
            - echo "nothing"

    requirements:
      run:
        - ${{ pin_subpackage('tinygrad', exact=True) }}
    tests:
      - requirements:
          run:
            # minimal
            - pytest
            - pytest-xdist
            - hypothesis
            - ml_dtypes
            - numpy
            - pytorch
            - z3-solver
            # optional
            - blobfile
            - boto3
            - bottle
            # https://github.com/conda-forge/staged-recipes/issues/29179
            # - ggml-python
            - if: unix
              then:
                # not available on windows yet
                - jax
            - librosa
            - networkx
            - nibabel
            - onnx2torch
            - onnx
            - onnxruntime
            - pandas
            - py-opencv
            - pybind11
            - pycocotools
            - pillow
            - safetensors
            - sentencepiece
            - tabulate
            - tiktoken
            - tqdm
            - transformers
        files:
          source:
            - examples/
            - extra/
            - test/
        script:
          interpreter: python
          content:
            - 'import pytest'
            - 'import sys'
            - 'tests_to_skip = "_not_a_real_test"'
            - if: linux
              then:
                # fails `assert Device.DEFAULT in failed_platforms`
                - 'tests_to_skip += " or (TestLinearizerFailures and test_failure_27)"'
                # Causes `Fatal Python error: Bus error` + segfault
                - 'tests_to_skip += " or testCopySHMtoDefault"'
                # RuntimeError: Attempting to relocate against an undefined symbol 'fmaxf'
                - 'tests_to_skip += " or test_backward_sum_acc_dtype"'
                # cannot load onnx model from some temporary location
                - 'tests_to_skip += " or (TestQuantizeOnnxCPU and test_quant_128)"'
            - if: linux and cuda_compiler_version != "None"
              then:
                # RuntimeError: Attribute list does not match Module context!
                - 'tests_to_skip += " or test_bf16_disk_write_read"'
            - if: osx or (linux and cuda_compiler_version != "None")
              then:
                # beam-related stuff seems to hang on metal, OOM/crash with CUDA
                - 'tests_to_skip += " or TestBEAM or test_beam"'
            - if: osx
              then:
                # another apparent hang
                - 'tests_to_skip += " or test_getotherprocess"'
                # forking too early
                - 'tests_to_skip += " or test_putotherprocess"'
                # running into weakly-referenced object no longer exists
                - 'tests_to_skip += " or TestGC or test_gc"'
            - if: osx and arm64
              then:
                - 'tests_to_skip += " or (TestOpsUint8 and test_cast)"'
            - if: osx and x86_64
              then:
                # signed int32 overflow
                - 'tests_to_skip += " or test_float_midcast_int32"'
                # tinygrad.codegen.kernel.KernelOptError: must have tensor cores or TC=2
                - 'tests_to_skip += " or test_unmerged_ifs"'
                # RuntimeError: The MPS backend is supported on MacOS 14.0+
                - 'tests_to_skip += " or test_torch_interop"'
                # element mismatch in array
                - 'tests_to_skip += " or test_conv2d_fused"'
                # flaky hypothesis test
                - 'tests_to_skip += " or test_float_cast_to_unsigned"'
                # less operations on CPU than expected on metal, apparently
                - 'tests_to_skip += " or test_jit_split_simple"'

            # test suite in emulation on aarch is super slow, skip it there
            - if: unix and build_platform == host_platform
              then:
                - 'sys.exit(pytest.main(["-v", "test/", "-k", f"not ({tests_to_skip})", "--durations=50", "--maxfail=20"]))'

about:
  homepage: https://github.com/tinygrad/tinygrad
  summary: 'You like pytorch? You like micrograd? You love tinygrad! ❤️'
  description: |
    This may not be the best deep learning framework, but it is a deep learning framework.

    Due to its extreme simplicity, it aims to be the easiest framework to add new accelerators to,
    with support for both inference and training. If XLA is CISC, tinygrad is RISC.
  license: MIT AND BSD-3-Clause
  license_file:
    - LICENSE
    - applegpu_LICENSE
  documentation: https://docs.tinygrad.org/
  repository: https://github.com/tinygrad/tinygrad

extra:
  recipe-maintainers:
    - h-vetinari
  feedstock-name: tinygrad
