context:
  version: "0.10.3"
  build_number: 0
  # see github.com/conda-forge/conda-forge.github.io/issues/1059 for naming discussion
  proc_type: ${{ "cuda" ~ cuda_compiler_version | version_to_buildstring if cuda_compiler_version != "None" else "cpu" }}

recipe:
  name: tinygrad
  version: ${{ version }}

source:
  url: https://github.com/tinygrad/tinygrad/archive/refs/tags/v${{ version }}.tar.gz
  sha256: ea91abf82ca5e89f15e0c0e71d222df8c037d225cc12651cb661fb77b2281ede
  patches:
    - patches/0001-point-to-our-own-llvmdev.patch
    # backport https://github.com/tinygrad/tinygrad/pull/12470 to make tests runnable out-of-tree
    - patches/0002-move-frontend-dir-to-nn-pr-12470.patch
    # see more clearly what's happening in mnist test
    - patches/0003-update-accuracy-for-every-step-in-beautiful_mnist.py.patch

build:
  number: ${{ build_number }}
  string: ${{ proc_type }}_py${{ python | version_to_buildstring }}_h${{ hash }}_${{ build_number }}
  skip:
    - match(python, "<3.10")

outputs:
  - package:
      name: tinygrad
    build:
      script: python -m pip install . -vv
    requirements:
      build:
        - if: build_platform != target_platform
          then:
            - python
            - cross-python_${{ target_platform }}
        # this is just here so smithy creates separate jobs for the CUDA variants
        - if: cuda_compiler_version != "None"
          then:
            - ${{ stdlib("c") }}
            - ${{ compiler("cxx") }}
            - ${{ compiler("cuda") }}
      host:
        - python
        - pip
        - setuptools
      run:
        - python
        - llvmdev
        - clangxx_${{ target_platform }}
        - if: cuda_compiler_version != "None"
          then:
            - cuda-nvcc_${{ target_platform }}
            - if: linux
              then:
                - triton
    tests:
      - python:
          imports:
            - tinygrad
            - tinygrad.runtime.autogen.llvm
          pip_check: true

  - package:
      name: tinygrad-tests
    build:
      # splitting off what conda packages under etc/conda/test-files when using test.files.source
      script:
        - if: unix
          # silly: https://github.com/prefix-dev/rattler-build/pull/1367
          then: true
          else:
            # remove symlinks in sources on windows by copying; delete target first
            - del /s /q extra\accel\ane\lib\h11ane.h
            - del /s /q extra\accel\ane\lib\entitlements.xml
            - del /s /q extra\accel\ane\2_compile\ane.py
            - del /s /q extra\accel\ane\2_compile\aneregs.json
            - copy /Y extra\accel\ane\3_run\h11ane.h extra\accel\ane\lib\h11ane.h
            - copy /Y extra\accel\ane\3_run\entitlements.xml extra\accel\ane\lib\entitlements.xml
            - copy /Y extra\accel\ane\lib\ane.py extra\accel\ane\2_compile\ane.py
            - copy /Y extra\accel\ane\lib\aneregs.json extra\accel\ane\2_compile\aneregs.json

    requirements:
      run:
        - ${{ pin_subpackage('tinygrad', exact=True) }}
    tests:
      - requirements:
          run:
            # minimal
            - pytest
            - pytest-xdist
            - hypothesis
            - ml_dtypes
            - numpy
            - pytorch
            - z3-solver
            # optional
            - blobfile
            - boto3
            - bottle
            - if: x86_64
              then:
                - capstone
            # https://github.com/conda-forge/staged-recipes/issues/29179
            # - ggml-python
            - if: unix
              then:
                # not available on windows yet
                - jax
            - librosa
            - networkx
            - nibabel
            - onnx2torch
            - onnx
            - onnxruntime
            - pandas
            - py-opencv
            - pybind11
            - pycocotools
            - pillow
            - safetensors
            - sentencepiece
            - tabulate
            - tiktoken
            - tqdm
            - transformers
        files:
          source:
            - examples/
            - extra/
            - test/
        script:
          interpreter: python
          content:
            - 'import pytest'
            - 'import sys'
            - 'tests_to_skip = "_not_a_real_test"'
            - if: linux
              then:
                # fails `assert Device.DEFAULT in failed_platforms`
                - 'tests_to_skip += " or (TestLinearizerFailures and test_failure_27)"'
                # Causes `Fatal Python error: Bus error` + segfault
                - 'tests_to_skip += " or testCopySHMtoDefault"'
                # RuntimeError: Attempting to relocate against an undefined symbol 'fmaxf'
                - 'tests_to_skip += " or test_backward_sum_acc_dtype"'
                # cannot load onnx model from some temporary location
                - 'tests_to_skip += " or (TestQuantizeOnnxCPU and test_quant_128)"'
            - if: linux and cuda_compiler_version != "None"
              then:
                # RuntimeError: Attribute list does not match Module context!
                - 'tests_to_skip += " or test_bf16_disk_write_read"'
            - if: osx
              then:
                # error: use of undefined value '@.const.pickledata.7893608704'
                - 'tests_to_skip += " or (TestWhisper and (test_transcribe_batch21 or test_transcribe_file1))"'
                # signed int32 overflow
                - 'tests_to_skip += " or test_float_midcast_int32"'
                # tinygrad.codegen.kernel.KernelOptError: must have tensor cores or TC=2
                - 'tests_to_skip += " or test_unmerged_ifs"'
                # RuntimeError: The MPS backend is supported on MacOS 14.0+
                - 'tests_to_skip += " or test_torch_interop"'
                #  nan location mismatch in comparison
                - 'tests_to_skip += " or test_conv2d_fused"'
                # flaky hypothesis test
                - 'tests_to_skip += " or test_float_cast_to_unsigned"'
                # beam-related stuff seems to hang
                - 'tests_to_skip += " or TestBEAM or test_beam"'
                # another apparent hang
                - 'tests_to_skip += " or test_getotherprocess"'
                - 'tests_to_skip += " or test_putotherprocess"'

            # test suite in emulation on aarch is super slow, skip it there
            - if: unix and build_platform == target_platform
              then:
                - 'sys.exit(pytest.main(["-v", "test/", "-k", f"not ({tests_to_skip})", "--durations=50"]))'

      # simpler test that only runs one example;
      # windows cannot run test suite yet due to lack of jax, minimal test on aarch due to emulation
      - files:
          source:
            - examples/
        script:
          # reduce number of steps; we just need to show that it's functional
          # however, accuracy should at least trend in the right direction
          - if: unix
            then:
              - export STEPS=25
              - export TARGET_EVAL_ACC_PCT=90
            else:
              - set "STEPS=25"
              - set "TARGET_EVAL_ACC_PCT=90"
          - if: aarch64
            then:
              # reduce number of steps in emulation
              - export STEPS=5
              - unset TARGET_EVAL_ACC_PCT
          - python ./examples/beautiful_mnist.py

about:
  homepage: https://github.com/tinygrad/tinygrad
  summary: 'You like pytorch? You like micrograd? You love tinygrad! ❤️'
  description: |
    This may not be the best deep learning framework, but it is a deep learning framework.

    Due to its extreme simplicity, it aims to be the easiest framework to add new accelerators to,
    with support for both inference and training. If XLA is CISC, tinygrad is RISC.
  license: MIT
  license_file: LICENSE
  documentation: https://docs.tinygrad.org/
  repository: https://github.com/tinygrad/tinygrad

extra:
  recipe-maintainers:
    - h-vetinari
  feedstock-name: tinygrad
