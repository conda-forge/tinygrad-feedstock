context:
  version: "0.10.3"
  build_number: 0
  # see github.com/conda-forge/conda-forge.github.io/issues/1059 for naming discussion
  proc_type: ${{ "cuda" ~ cuda_compiler_version | version_to_buildstring if cuda_compiler_version != "None" else "cpu" }}
  tests_to_skip: >
    _not_a_real_test
    ${{ "fails `assert Device.DEFAULT in failed_platforms`" if 0 }}
    ${{ " or (TestLinearizerFailures and test_failure_27)" if linux }}
    ${{ "Causes `Fatal Python error: Bus error` + segfault" if 0 }}
    ${{ " or testCopySHMtoDefault" if linux }}
    ${{ "`RuntimeError: Attempting to relocate against an undefined symbol 'fmaxf'" if 0 }}
    ${{ " or test_backward_sum_acc_dtype" if linux }}
    ${{ "RuntimeError: Attribute list does not match Module context!" if 0 }}
    ${{ " or test_bf16_disk_write_read" if linux and cuda_compiler_version != "None" }}
    ${{ "`error: use of undefined value '@.const.pickledata.7893608704'`" if 0 }}
    ${{ " or (TestWhisper and (test_transcribe_batch21 or test_transcribe_file1))" if osx }}
    ${{ "signed int32 overflow" if 0 }}
    ${{ " or test_float_midcast_int32" if osx }}
    ${{ "tinygrad.codegen.kernel.KernelOptError: must have tensor cores or TC=2" if 0 }}
    ${{ " or test_unmerged_ifs" if osx }}

recipe:
  name: tinygrad
  version: ${{ version }}

source:
  url: https://github.com/tinygrad/tinygrad/archive/refs/tags/v${{ version }}.tar.gz
  sha256: ea91abf82ca5e89f15e0c0e71d222df8c037d225cc12651cb661fb77b2281ede

build:
  number: ${{ build_number }}
  string: ${{ proc_type }}_py${{ python | version_to_buildstring }}_h${{ hash }}_${{ build_number }}
  skip:
    - match(python, "<3.10")

outputs:
  - package:
      name: tinygrad
    build:
      script: python -m pip install . -vv
    requirements:
      build:
        - if: build_platform != target_platform
          then:
            - python
            - cross-python_${{ target_platform }}
        # this is just here so smithy creates separate jobs for the CUDA variants
        - if: cuda_compiler_version != "None"
          then:
            - ${{ stdlib("c") }}
            - ${{ compiler("cxx") }}
            - ${{ compiler("cuda") }}
      host:
        - python
        - pip
        - setuptools
      run:
        - python
        - if: win
          then:
            - clang-cl_${{ target_platform }}
          else:
            - clangxx_${{ target_platform }}
        - if: cuda_compiler_version != "None"
          then:
            - cuda-nvcc_${{ target_platform }}
            - if: linux
              then:
                - triton
    tests:
      - python:
          imports:
            - tinygrad
          pip_check: true

  - package:
      name: tinygrad-tests
    build:
      # splitting off what conda packages under etc/conda/test-files when using test.files.source
      script:
        - if: unix
          # silly: https://github.com/prefix-dev/rattler-build/pull/1367
          then: true
          else:
            # remove symlinks in sources on windows by copying
            - copy /Y extra\accel\ane\3_run\h11ane.h extra\accel\ane\lib\h11ane.h
            - copy /Y extra\accel\ane\3_run\entitlements.xml extra\accel\ane\lib\entitlements.xml
            - copy /Y extra\accel\ane\lib\ane.py extra\accel\ane\2_compile\ane.py
            - copy /Y extra\accel\ane\lib\aneregs.json extra\accel\ane\2_compile\aneregs.json

    requirements:
      run:
        - ${{ pin_subpackage('tinygrad', exact=True) }}
    tests:
      - requirements:
          run:
            # minimal
            - pytest
            - pytest-xdist
            - hypothesis
            - ml_dtypes
            - numpy
            - pytorch
            - z3-solver
            # optional
            - blobfile
            - boto3
            - bottle
            - if: x86_64
              then:
                - capstone
            # https://github.com/conda-forge/staged-recipes/issues/29179
            # - ggml-python
            - if: unix
              then:
                # not available on windows yet
                - jax
            - librosa
            - networkx
            - nibabel
            - onnx2torch
            - onnx
            - onnxruntime
            - pandas
            - py-opencv
            - pycocotools
            - pillow
            - safetensors
            - sentencepiece
            - tabulate
            - tiktoken
            - tqdm
            - transformers
        files:
          source:
            - examples/
            - extra/
            - test/
        script:
          # test suite in emulation on aarch is super slow
          - if: unix and build_platform == target_platform
            then:
              - pytest -n 2 -v test/ -k "not (${{ tests_to_skip }})"
          - if: win
            then:
              # windows cannot run tests yet due to lack of jax
              - python examples/beautiful_mnist.py
          - echo "just something so aarch doesn't have an empty script section"

about:
  homepage: https://github.com/tinygrad/tinygrad
  summary: 'You like pytorch? You like micrograd? You love tinygrad! ❤️'
  description: |
    This may not be the best deep learning framework, but it is a deep learning framework.

    Due to its extreme simplicity, it aims to be the easiest framework to add new accelerators to,
    with support for both inference and training. If XLA is CISC, tinygrad is RISC.
  license: MIT
  license_file: LICENSE
  documentation: https://docs.tinygrad.org/
  repository: https://github.com/tinygrad/tinygrad

extra:
  recipe-maintainers:
    - h-vetinari
  feedstock-name: tinygrad
