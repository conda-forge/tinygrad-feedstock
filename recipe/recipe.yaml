context:
  version: "0.11.1dev"
  build_number: 0
  # see github.com/conda-forge/conda-forge.github.io/issues/1059 for naming discussion
  proc_type: ${{ "cuda" ~ cuda_compiler_version | version_to_buildstring if cuda_compiler_version != "None" else "cpu" }}

recipe:
  name: tinygrad
  version: ${{ version }}

source:
  git: https://github.com/tinygrad/tinygrad.git
  rev: "54af29dbdb6c266bae2590e68d4a0b2c5e61279c"
  # url: https://github.com/tinygrad/tinygrad/archive/refs/tags/v${{ version }}.tar.gz
  # sha256: e5e7b0f2319a8cc8e5de14dbc6ee91a403efd2414681b874b526a8b39d7fa86f
  patches:
    - patches/0001-point-to-our-own-llvmdev.patch
    # see more clearly what's happening in mnist test
    - patches/0002-update-accuracy-for-every-step-in-beautiful_mnist.py.patch

build:
  number: ${{ build_number }}
  string: ${{ proc_type }}_py${{ python | version_to_buildstring }}_h${{ hash }}_${{ build_number }}

outputs:
  - package:
      name: tinygrad
    build:
      script: python -m pip install . -vv
    requirements:
      build:
        - if: build_platform != target_platform
          then:
            - python
            - cross-python_${{ target_platform }}
        # this is just here so smithy creates separate jobs for the CUDA variants
        - if: cuda_compiler_version != "None"
          then:
            - ${{ stdlib("c") }}
            - ${{ compiler("cxx") }}
            - ${{ compiler("cuda") }}
      host:
        - python
        - pip
        - setuptools
      run:
        - python
        - llvmdev
        - clangxx_${{ target_platform }}
        # ensure `bin/clang` symlink is present
        - clang
        - if: cuda_compiler_version != "None"
          then:
            - cuda-nvcc_${{ target_platform }}
            - if: linux
              then:
                - triton
    tests:
      - python:
          imports:
            - tinygrad
            - tinygrad.runtime.autogen.llvm
          pip_check: true

  - package:
      name: tinygrad-tests
    build:
      # splitting off what conda packages under etc/conda/test-files when using test.files.source
      script:
        - if: unix
          # silly: https://github.com/prefix-dev/rattler-build/pull/1367
          then: true
          else:
            - echo "nothing"

    requirements:
      run:
        - ${{ pin_subpackage('tinygrad', exact=True) }}
    tests:
      - requirements:
          run:
            # minimal
            - pytest
            - pytest-xdist
            - hypothesis
            - ml_dtypes
            - numpy
            - pytorch
            - z3-solver
            # optional
            - blobfile
            - boto3
            - bottle
            - if: x86_64
              then:
                - capstone
            # https://github.com/conda-forge/staged-recipes/issues/29179
            # - ggml-python
            - if: unix
              then:
                # not available on windows yet
                - jax
            - librosa
            - networkx
            - nibabel
            - onnx2torch
            - onnx
            - onnxruntime
            - pandas
            - py-opencv
            - pybind11
            - pycocotools
            - pillow
            - safetensors
            - sentencepiece
            - tabulate
            - tiktoken
            - tqdm
            - transformers
        files:
          source:
            - examples/
            - extra/
            - test/
        script:
          interpreter: python
          content:
            - 'import pytest'
            - 'import sys'
            - 'tests_to_skip = "_not_a_real_test"'
            - if: linux
              then:
                # fails `assert Device.DEFAULT in failed_platforms`
                - 'tests_to_skip += " or (TestLinearizerFailures and test_failure_27)"'
                # Causes `Fatal Python error: Bus error` + segfault
                - 'tests_to_skip += " or testCopySHMtoDefault"'
                # RuntimeError: Attempting to relocate against an undefined symbol 'fmaxf'
                - 'tests_to_skip += " or test_backward_sum_acc_dtype"'
                # cannot load onnx model from some temporary location
                - 'tests_to_skip += " or (TestQuantizeOnnxCPU and test_quant_128)"'
            - if: linux and cuda_compiler_version != "None"
              then:
                # RuntimeError: Attribute list does not match Module context!
                - 'tests_to_skip += " or test_bf16_disk_write_read"'
            - if: osx
              then:
                # signed int32 overflow
                - 'tests_to_skip += " or test_float_midcast_int32"'
                # tinygrad.codegen.kernel.KernelOptError: must have tensor cores or TC=2
                - 'tests_to_skip += " or test_unmerged_ifs"'
                # RuntimeError: The MPS backend is supported on MacOS 14.0+
                - 'tests_to_skip += " or test_torch_interop"'
                # element mismatch in array
                - 'tests_to_skip += " or test_conv2d_fused"'
                # flaky hypothesis test
                - 'tests_to_skip += " or test_float_cast_to_unsigned"'
                # beam-related stuff seems to hang
                - 'tests_to_skip += " or TestBEAM or test_beam"'
                # another apparent hang
                - 'tests_to_skip += " or test_getotherprocess"'
                # forking too early
                - 'tests_to_skip += " or test_putotherprocess"'
                # running into weakly-referenced object no longer exists
                - 'tests_to_skip += " or TestGC or test_gc"'

            # test suite in emulation on aarch is super slow, skip it there
            - if: unix and build_platform == target_platform
              then:
                - 'sys.exit(pytest.main(["-v", "test/", "-k", f"not ({tests_to_skip})", "--durations=50"]))'

      # simpler test that only runs one example;
      # windows cannot run test suite yet due to lack of jax, minimal test on aarch due to emulation
      - files:
          source:
            - examples/
        script:
          # reduce number of steps; we just need to show that it's functional
          # however, accuracy should at least trend in the right direction
          - if: unix
            then:
              - export STEPS=25
              - export TARGET_EVAL_ACC_PCT=90
            else:
              - set "STEPS=25"
              - set "TARGET_EVAL_ACC_PCT=90"
              # remove extraneous clang's in image, c.f.
              # https://github.com/conda-forge/conda-forge-ci-setup-feedstock/pull/408
              - rmdir /s /q "C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC\Tools\Llvm"
              - rmdir /s /q "C:\Program Files\LLVM"
          - if: aarch64
            then:
              # reduce number of steps in emulation
              - export STEPS=3
              - export TARGET_EVAL_ACC_PCT=30
          - if: osx
            then:
              # loss decreases as expected, but accuracy appears to be mis-computed
              - unset TARGET_EVAL_ACC_PCT
          - python ./examples/beautiful_mnist.py

about:
  homepage: https://github.com/tinygrad/tinygrad
  summary: 'You like pytorch? You like micrograd? You love tinygrad! ❤️'
  description: |
    This may not be the best deep learning framework, but it is a deep learning framework.

    Due to its extreme simplicity, it aims to be the easiest framework to add new accelerators to,
    with support for both inference and training. If XLA is CISC, tinygrad is RISC.
  license: MIT
  license_file: LICENSE
  documentation: https://docs.tinygrad.org/
  repository: https://github.com/tinygrad/tinygrad

extra:
  recipe-maintainers:
    - h-vetinari
  feedstock-name: tinygrad
