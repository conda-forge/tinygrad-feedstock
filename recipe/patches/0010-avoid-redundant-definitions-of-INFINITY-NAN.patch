From d163278dca0bc777411fc3eb014d3c291f0f533c Mon Sep 17 00:00:00 2001
From: "H. Vetinari" <h.vetinari@gmx.com>
Date: Fri, 13 Feb 2026 09:47:14 +1100
Subject: [PATCH 10/10] avoid redundant definitions of INFINITY/NAN

---
 extra/gemm/max_kernels/nv.fp16_fp16_fp16.2_stage.cu            | 2 --
 extra/gemm/max_kernels/nv.fp16_fp16_fp16.3_stage.cu            | 2 --
 extra/gemm/max_kernels/nv.fp16_fp16_fp16.3_stage_swizzled.cu   | 2 --
 extra/gemm/max_kernels/nv.fp16_fp16_fp16.max.cu                | 2 --
 extra/gemm/max_kernels/nv.fp16_fp16_fp16.no_xor.cu             | 2 --
 extra/gemm/max_kernels/nv.fp16_fp32_fp16.hcopt.cu              | 2 --
 .../nv.fp16_fp32_fp32.2_stage_swizzled_smem_input.cu           | 2 --
 extra/gemm/max_kernels/nv.fp16_fp32_fp32.flat_smem_input.cu    | 2 --
 extra/gemm/max_kernels/nv.fp16_fp32_fp32.max.cu                | 2 --
 .../gemm/max_kernels/nv.fp16_fp32_fp32.swizzled_smem_input.cu  | 2 --
 tinygrad/renderer/cstyle.py                                    | 3 +--
 11 files changed, 1 insertion(+), 22 deletions(-)

diff --git a/extra/gemm/max_kernels/nv.fp16_fp16_fp16.2_stage.cu b/extra/gemm/max_kernels/nv.fp16_fp16_fp16.2_stage.cu
index 32879b1e6..a29402093 100644
--- a/extra/gemm/max_kernels/nv.fp16_fp16_fp16.2_stage.cu
+++ b/extra/gemm/max_kernels/nv.fp16_fp16_fp16.2_stage.cu
@@ -1,5 +1,3 @@
-#define INFINITY (__int_as_float(0x7f800000))
-#define NAN (__int_as_float(0x7fffffff))
 #include <cuda_fp16.h>
 #include <cuda_pipeline.h>
 #define N_PAD 132
diff --git a/extra/gemm/max_kernels/nv.fp16_fp16_fp16.3_stage.cu b/extra/gemm/max_kernels/nv.fp16_fp16_fp16.3_stage.cu
index 206fd9e32..4cdde04bd 100644
--- a/extra/gemm/max_kernels/nv.fp16_fp16_fp16.3_stage.cu
+++ b/extra/gemm/max_kernels/nv.fp16_fp16_fp16.3_stage.cu
@@ -1,5 +1,3 @@
-#define INFINITY (__int_as_float(0x7f800000))
-#define NAN (__int_as_float(0x7fffffff))
 #include <cuda_fp16.h>
 #include <cuda_pipeline.h>
 #define N_PAD 132
diff --git a/extra/gemm/max_kernels/nv.fp16_fp16_fp16.3_stage_swizzled.cu b/extra/gemm/max_kernels/nv.fp16_fp16_fp16.3_stage_swizzled.cu
index 0352666b2..108edb25a 100644
--- a/extra/gemm/max_kernels/nv.fp16_fp16_fp16.3_stage_swizzled.cu
+++ b/extra/gemm/max_kernels/nv.fp16_fp16_fp16.3_stage_swizzled.cu
@@ -1,5 +1,3 @@
-#define INFINITY (__int_as_float(0x7f800000))
-#define NAN (__int_as_float(0x7fffffff))
 #include <cuda_fp16.h>
 #include <cuda_pipeline.h>
 #define N_PAD 132
diff --git a/extra/gemm/max_kernels/nv.fp16_fp16_fp16.max.cu b/extra/gemm/max_kernels/nv.fp16_fp16_fp16.max.cu
index 588f6b3c0..93812e9c4 100644
--- a/extra/gemm/max_kernels/nv.fp16_fp16_fp16.max.cu
+++ b/extra/gemm/max_kernels/nv.fp16_fp16_fp16.max.cu
@@ -1,5 +1,3 @@
-#define INFINITY (__int_as_float(0x7f800000))
-#define NAN (__int_as_float(0x7fffffff))
 #include <cuda_fp16.h>
 #include <cuda_pipeline.h>
 #define SMEM_N_WIDTH 136
diff --git a/extra/gemm/max_kernels/nv.fp16_fp16_fp16.no_xor.cu b/extra/gemm/max_kernels/nv.fp16_fp16_fp16.no_xor.cu
index 962a21945..5934a82cb 100644
--- a/extra/gemm/max_kernels/nv.fp16_fp16_fp16.no_xor.cu
+++ b/extra/gemm/max_kernels/nv.fp16_fp16_fp16.no_xor.cu
@@ -1,5 +1,3 @@
-#define INFINITY (__int_as_float(0x7f800000))
-#define NAN (__int_as_float(0x7fffffff))
 #include <cuda_fp16.h>
 #include <cuda_pipeline.h>
 #define SMEM_N_WIDTH 136
diff --git a/extra/gemm/max_kernels/nv.fp16_fp32_fp16.hcopt.cu b/extra/gemm/max_kernels/nv.fp16_fp32_fp16.hcopt.cu
index f17f8693a..3654bf585 100644
--- a/extra/gemm/max_kernels/nv.fp16_fp32_fp16.hcopt.cu
+++ b/extra/gemm/max_kernels/nv.fp16_fp32_fp16.hcopt.cu
@@ -1,5 +1,3 @@
-#define INFINITY (__int_as_float(0x7f800000))
-#define NAN (__int_as_float(0x7fffffff))
 #include <cuda_fp16.h>
 struct __align__(8) half4 { half x, y, z, w; }; __device__ half4 make_half4(half x, half y, half z, half w) { half4 r={x, y, z, w}; return r; }
 struct __align__(16) half8 { half x, y, z, w, a, b, c, d; }; __device__ half8 make_half8(half x, half y, half z, half w, half a, half b, half c, half d) { half8 r={x, y, z, w, a, b, c, d}; return r; }
diff --git a/extra/gemm/max_kernels/nv.fp16_fp32_fp32.2_stage_swizzled_smem_input.cu b/extra/gemm/max_kernels/nv.fp16_fp32_fp32.2_stage_swizzled_smem_input.cu
index 689da87d3..45e79ce33 100644
--- a/extra/gemm/max_kernels/nv.fp16_fp32_fp32.2_stage_swizzled_smem_input.cu
+++ b/extra/gemm/max_kernels/nv.fp16_fp32_fp32.2_stage_swizzled_smem_input.cu
@@ -1,5 +1,3 @@
-#define INFINITY (__int_as_float(0x7f800000))
-#define NAN (__int_as_float(0x7fffffff))
 #include <cuda_fp16.h>
 #include <cuda_pipeline.h>
 #define N_PAD 132
diff --git a/extra/gemm/max_kernels/nv.fp16_fp32_fp32.flat_smem_input.cu b/extra/gemm/max_kernels/nv.fp16_fp32_fp32.flat_smem_input.cu
index cf9c2ca39..b592f33c0 100644
--- a/extra/gemm/max_kernels/nv.fp16_fp32_fp32.flat_smem_input.cu
+++ b/extra/gemm/max_kernels/nv.fp16_fp32_fp32.flat_smem_input.cu
@@ -1,5 +1,3 @@
-#define INFINITY (__int_as_float(0x7f800000))
-#define NAN (__int_as_float(0x7fffffff))
 #include <cuda_fp16.h>
 #include <cuda_pipeline.h>
 #define N_PAD 132
diff --git a/extra/gemm/max_kernels/nv.fp16_fp32_fp32.max.cu b/extra/gemm/max_kernels/nv.fp16_fp32_fp32.max.cu
index b21e86773..6551f2bf4 100644
--- a/extra/gemm/max_kernels/nv.fp16_fp32_fp32.max.cu
+++ b/extra/gemm/max_kernels/nv.fp16_fp32_fp32.max.cu
@@ -1,5 +1,3 @@
-#define INFINITY (__int_as_float(0x7f800000))
-#define NAN (__int_as_float(0x7fffffff))
 #include <cuda_fp16.h>
 #include <cuda_pipeline.h>
 #define N_PAD 132
diff --git a/extra/gemm/max_kernels/nv.fp16_fp32_fp32.swizzled_smem_input.cu b/extra/gemm/max_kernels/nv.fp16_fp32_fp32.swizzled_smem_input.cu
index 8e168b893..bd2b2577a 100644
--- a/extra/gemm/max_kernels/nv.fp16_fp32_fp32.swizzled_smem_input.cu
+++ b/extra/gemm/max_kernels/nv.fp16_fp32_fp32.swizzled_smem_input.cu
@@ -1,5 +1,3 @@
-#define INFINITY (__int_as_float(0x7f800000))
-#define NAN (__int_as_float(0x7fffffff))
 #include <cuda_fp16.h>
 #include <cuda_pipeline.h>
 #define N_PAD 132
diff --git a/tinygrad/renderer/cstyle.py b/tinygrad/renderer/cstyle.py
index 65d12d0e0..27ce1d763 100644
--- a/tinygrad/renderer/cstyle.py
+++ b/tinygrad/renderer/cstyle.py
@@ -425,8 +425,7 @@ class CUDARenderer(CStyleLanguage):
 
   def render_kernel(self, function_name, kernel, bufs, uops, prefix=None):
     # TODO: why is dtypes.bfloat16.name == "__bf16"? would be easier not override dtypes.name
-    prefix = ["#define INFINITY (__int_as_float(0x7f800000))", "#define NAN (__int_as_float(0x7fffffff))",
-              "template <class T, class F> __device__ __forceinline__ T tg_bitcast(F v) { union U { F f; T t; }; U u; u.f = v; return u.t; }"]
+    prefix = ["template <class T, class F> __device__ __forceinline__ T tg_bitcast(F v) { union U { F f; T t; }; U u; u.f = v; return u.t; }"]
     used_dtypes = uops_to_dtypes(uops)
     if any(dt.scalar() in dtypes.fp8s for dt in used_dtypes): prefix.append("#include <cuda_fp8.h>")
     if any(dt.scalar() == dtypes.half for dt in used_dtypes): prefix.append("#include <cuda_fp16.h>")
